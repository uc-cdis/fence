"""
At the time of writing, Prometheus metrics out of the box can't be reset between each
unit test. To be able to write independent unit tests, we have to manually save the "previous
state" (see `prometheus_metrics_before` fixture) and compare it to the new state. This involves
manually parsing the "previous state" (a python object) and the "current state" (raw text) into
the same format so they can be compared:
{ "name": "", "labels": {}, "value": 0 }

The utility functions below can be used to check that the expected metrics have been recorded,
while discarding any previous metrics.

https://stackoverflow.com/questions/73198616/how-do-i-reset-a-prometheus-python-client-python-runtime-between-pytest-test-fun
"""


import os
import shutil

import pytest


@pytest.fixture(autouse=True, scope="session")
def reset_prometheus_metrics():
    """
    Delete the prometheus files after all the tests have run.
    Without this, when running the tests locally, we would keep reading the metrics from
    previous test runs.
    So why not run this in-between the unit tests instead of the `assert_prometheus_metrics`
    logic? Because it doesn't work, the prometheus client also keeps the state, and the mismatch
    causes errors. This only works when the client is reset too (new process)
    """
    assert os.environ.get("PROMETHEUS_CONFIGURED_FOR_TESTS"), "Prometheus is not configured correctly. Make sure to run the tests using `tests/ci_commands_script.sh`."

    yield

    folder = os.environ["PROMETHEUS_MULTIPROC_DIR"]
    for filename in os.listdir(folder):
        file_path = os.path.join(folder, filename)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.unlink(file_path)
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
        except Exception as e:
            print(f"Failed to delete Prometheus metrics file '{file_path}': {e}")


def _diff_new_metrics_from_old_metrics(new_metrics, old_metrics):
    """
    Return a list of "current metrics" by comparing the "new metrics" (current state) to the "old metrics" (previous state).

    Input metric format example: {
        'gen3_fence_login_total{client_id="test_azp",fence_idp="shib",idp="test_idp",shib_idp="university",user_sub="123"}': 2.0,
        'gen3_fence_login_total{client_id="test_azp",fence_idp="None",idp="all",shib_idp="None",user_sub="123"}': 3.0,
    }

    Functionality example:
        old_metrics = { 'counter1': 2, 'counter2': 2, 'gauge1': 1 }
        new_metrics = { 'counter1': 1, 'counter3': 1 }
        Returned value = [
            ('counter1', 1) (difference between 2 and 1),
            ('counter3', 1)
        ] (counter2 and gauge1 omitted since they are not part of the current state)

    Args:
        new_metrics (dict): format { <unparsed metric name and labels>: <metric value> }
        old_metrics (dict): format { <unparsed metric name and labels>: <metric value> }
    }

    Return:
        list<tuple<long metric name, metric value>>
    """

    def metric_is_gauge(metric_name):
        return not metric_name.endswith("_total") and not metric_name.endswith(
            "_created"
        )

    diff = []
    for long_metric_name, old_value in old_metrics.items():
        # long_metric_name = metric name + labels (see example in docstring)
        metric_name = long_metric_name.split("{")[0]
        if long_metric_name not in new_metrics or metric_is_gauge(metric_name):
            # ignore all old metrics that are not also present in the new metrics
            continue
        # the metric value generated by the current test is the difference between the previous
        # value and the current value
        val = new_metrics[long_metric_name] - old_value
        if val != 0:
            diff.append((long_metric_name, val))
    for long_metric_name, new_value in new_metrics.items():
        metric_name = long_metric_name.split("{")[0]
        if metric_is_gauge(metric_name):  # all gauge metrics must be listed
            diff.append((long_metric_name, new_value))
        elif long_metric_name not in old_metrics:
            diff.append((long_metric_name, new_value))
    return diff


def _parse_raw_metrics_to_dict(text_metric):
    """
    Parse raw text metrics into a dictionary of metric (metric name + labels) to value,
    ignoring lines that are not metrics.

    Args:
        text_metric (str)
            Example:
                # TYPE gen3_fence_login_total counter
                gen3_fence_login_total{idp="test_idp",shib_idp="university",user_sub="123"} 2.0
                # HELP gen3_fence_presigned_url_total Fence presigned urls
                # TYPE gen3_fence_presigned_url_total counter
                gen3_fence_presigned_url_total{client_id="test_azp",drs="True",user_sub="123"} 1.0

    Return:
        dict<long metric name: metric value>
            Example:
                {
                    "gen3_fence_login_total{idp="test_idp",shib_idp="university",user_sub="123"}": 2.0,
                    "gen3_fence_presigned_url_total{client_id="test_azp",drs="True",user_sub="123"}": 1.0,
                }
    """
    if not text_metric:
        return {}
    return {
        " ".join(m.split(" ")[:-1]): float(m.split(" ")[-1])
        for m in text_metric.strip().split("\n")
        if not m.startswith("#")
    }


def _parse_raw_name_to_labels(text_metric_name):
    """
    Parse a raw metric name into a name and a dict of labels.

    Example:
        text_metric_name = `metric_name{param1="None",param2="upload",param3="['/test/path']"`
        Returned value = {
            "name": "metric_name",
            "labels": { "param1": "None", "param2": "upload", "param3": "['/test/path']" }
        }

    Args:
        text_metric (str)

    Returns:
        dict
    """
    name = text_metric_name.split("{")[0]
    labels = text_metric_name.split("{")[1].split("}")[0].split('",')
    labels = {l.split("=")[0]: l.split("=")[1].strip('"') for l in labels}
    return {"name": name, "labels": labels}


def assert_prometheus_metrics(
    previous_text_metrics, current_text_metrics, expected_metrics
):
    """
    Compare the previous state and the current state of prometheus metrics, and checks if the difference between the 2 is the same as the new metrics a test expects to have recorded.

    Expected: only provide labels we need to check for, the rest will be ignored

    Args:
        previous_text_metrics (str): previous state of prometheus metrics
        current_text_metrics (str): current state
            Example `previous_text_metrics` or `current_text_metrics`:
                # TYPE gen3_fence_login_total counter
                gen3_fence_login_total{idp="test_idp",shib_idp="university",user_sub="123"} 2.0
                # HELP gen3_fence_presigned_url_total Fence presigned urls
                # TYPE gen3_fence_presigned_url_total counter
                gen3_fence_presigned_url_total{acl="None",action="upload",authz="['/test/path']",bucket="s3://test-bucket",client_id="test_azp",drs="True",protocol="s3",user_sub="123"} 1.0
        expected_metrics (list<dict>): the expected difference between previous state and current state.
            Only provide the labels we need to check; omitted labels will be ignored even if they
            are present in the current state.
            Example: [
                {
                    'name': 'gen3_fence_login_total',
                    'labels': {
                        'idp': 'test_idp', 'shib_idp': 'university', 'user_sub': '123'
                    },
                    'value': 2.0
                }
            ]
    """
    old_metrics = _parse_raw_metrics_to_dict(previous_text_metrics)
    print("Old metrics:")
    for k, v in old_metrics.items():
        print(f"- {k} = {v}")

    new_metrics = _parse_raw_metrics_to_dict(current_text_metrics)
    print("Received metrics:")
    for k, v in new_metrics.items():
        print(f"- {k} = {v}")

    diff_metrics = _diff_new_metrics_from_old_metrics(new_metrics, old_metrics)
    current_metrics = []
    print("Diff:")
    for (metric_name, val) in diff_metrics:
        parsed_m = _parse_raw_name_to_labels(metric_name)
        parsed_m["value"] = val
        current_metrics.append(parsed_m)
        print(f"- {parsed_m}")

    print("Expecting metrics:")
    # check that for each metric+label combination, the value is identical to the expected value
    for expected_m in expected_metrics:
        found = False
        print(f"- {expected_m}")
        for current_m in current_metrics:  # look for the right metric
            if current_m["name"] != expected_m["name"]:
                continue
            # if the metric name is identical, check the labels
            right_labels = True
            for label_k, label_v in expected_m["labels"].items():
                if current_m["labels"].get(label_k) != str(label_v):
                    right_labels = False
                    break
            # if both the name and the labels are identical, this is the right metric:
            # check that the value is the same as expected
            if right_labels:
                assert (
                    current_m["value"] == expected_m["value"]
                ), f"Missing metric: {expected_m}"
                found = True
                break  # we found the right metric and it has the right value: moving on
        assert found, f"Missing metric: {expected_m}"
